{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral 7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-16 22:38:05.353468: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-16 22:38:05.537014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-16 22:38:05.616642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-16 22:38:05.638261: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-16 22:38:05.786028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-16 22:38:06.541200: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eaaad47f7d343c0a70b9889770a1357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f6fb81df3c4f39bcb9d8bbd49b4952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/137k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797e392e9ed44141985bb441379de386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f60f042107b43839af66a9eee21e0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f40403d8a904a12840269f9d934c93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from evaluate import load\n",
    "import time\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"OpenAssistant/oasst1\")\n",
    "\n",
    "# Model name\n",
    "model_name = \"mistralai/Mistral-7B-v0.3\"\n",
    "\n",
    "# Load the tokenizer for Mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,      # Add end-of-sequence token to the tokenizer\n",
    "    use_fast=True,           # Use the fast tokenizer implementation\n",
    "    padding_side='left'      # Pad sequences on the left side\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0929533c857d44aabc9226bd56ae4ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547a6eaa36134990b9c57e93826340dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a494d1c5fd474f1988a3398c9e4a0c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e161b347cadb4fa68a0fae9bead93689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5af358e59f74caa963d2fc809a1a330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cabd46bc9304fad8286dc26d371eeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284cec88e53e4e9c8fe1539528884add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3c5f7dcc5249aaa92c0bbf834eff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 3771469824\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model \n",
    "# Quantization configuration using bitsandbytes library\n",
    "compute_dtype = getattr(torch, \"bfloat16\")  # Set computation data type to bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")\n",
    "\n",
    "# Load the pre-trained model with the specified quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Apply quantization configuration\n",
    "    device_map=\"auto\"                # Automatically map layers to devices\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit (e.g., 4-bit) training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# lora\n",
    "# Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,             # Scaling factor for LoRA updates\n",
    "    lora_dropout=0.05,         # Dropout rate applied to LoRA layers\n",
    "    r=5,                      # Rank of the LoRA decomposition\n",
    "    bias=\"none\",               # No bias is added to the LoRA layers\n",
    "    task_type=\"CAUSAL_LM\",     # Specify the task as causal language modeling\n",
    "    target_modules=[           # Modules to apply LoRA to\n",
    "        'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "        'gate_proj', 'down_proj', 'up_proj'\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n",
    "# print how much does the model occupy in memory\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
       "        num_rows: 84437\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n",
       "        num_rows: 4401\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "#  Paso 1: Crear la columna 'conversations' en el dataset\n",
    "def create_conversations(split_dataset):\n",
    "    # Crear un diccionario para acceder r치pidamente a los mensajes por 'message_id'\n",
    "    message_dict = {msg['message_id']: msg for msg in split_dataset}\n",
    "    \n",
    "    # Lista para almacenar las conversaciones\n",
    "    conversations = []\n",
    "    \n",
    "    for msg in split_dataset:\n",
    "        if msg['role'] == 'assistant':\n",
    "            # Reconstruir la conversaci칩n desde el assistant hasta el prompter\n",
    "            conversation = []\n",
    "            current_msg = msg\n",
    "            while current_msg:\n",
    "                conversation.insert(0, current_msg['text'])\n",
    "                parent_id = current_msg['parent_id']\n",
    "                if parent_id and parent_id in message_dict:\n",
    "                    current_msg = message_dict[parent_id]\n",
    "                else:\n",
    "                    current_msg = None\n",
    "            conversations.append({'conversations': conversation})\n",
    "    \n",
    "    # Crear un nuevo dataset a partir de las conversaciones\n",
    "    new_dataset = Dataset.from_list(conversations)\n",
    "    return new_dataset\n",
    "\n",
    "# Procesar cada split para crear el campo 'conversations'\n",
    "new_train_dataset = create_conversations(dataset['train'])\n",
    "new_validation_dataset = create_conversations(dataset['validation'])\n",
    "\n",
    "# Crear un nuevo DatasetDict con los splits procesados\n",
    "new_dataset = DatasetDict({\n",
    "    'train': new_train_dataset,\n",
    "    'validation': new_validation_dataset\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5813570462496b89249fda1e36e50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c9fda5b5b54419825e99f3657835c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2756 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paso 2: Tokenizar el dataset utilizando la funci칩n de tu profesor\n",
    "def format_conversation(examples):\n",
    "    # Unir las conversaciones en un solo string\n",
    "    joined_conversations = [\"\\n\".join(conv) if isinstance(conv, list) else conv for conv in examples['conversations']]\n",
    "    \n",
    "    # Tokenizar las conversaciones unidas\n",
    "    tokenized = tokenizer(\n",
    "        joined_conversations,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convertir tensores a listas para evitar problemas\n",
    "    tokenized = {k: v.tolist() for k, v in tokenized.items()}\n",
    "    return tokenized\n",
    "\n",
    "# Aplicar la tokenizaci칩n al dataset\n",
    "tokenized_dataset = new_dataset.map(format_conversation, batched=True, remove_columns=[\"conversations\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Configurar el formato del dataset para PyTorch\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4: Definir los argumentos de entrenamiento\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./v2_mistral7b_results\",  # Directory for saving model checkpoints and logs\n",
    "    eval_strategy=\"steps\",                # Evaluation strategy: evaluate every few steps\n",
    "    do_eval=True,                         # Enable evaluation during training\n",
    "    optim=\"paged_adamw_8bit\",             # Use 8-bit AdamW optimizer for memory efficiency\n",
    "    per_device_train_batch_size=4,        # Batch size per device during training\n",
    "    gradient_accumulation_steps=2,        # Accumulate gradients over multiple steps\n",
    "    per_device_eval_batch_size=2,         # Batch size per device during evaluation\n",
    "    log_level=\"debug\",                    # Set logging level to debug for detailed logs\n",
    "    logging_steps=10,                     # Log metrics every 10 steps\n",
    "    learning_rate=1e-4,                   # Initial learning rate\n",
    "    eval_steps=25,                        # Evaluate the model every 25 steps\n",
    "    max_steps=100,                        # Total number of training steps\n",
    "    save_steps=25,                        # Save checkpoints every 25 steps\n",
    "    warmup_steps=25,                      # Number of warmup steps for learning rate scheduler\n",
    "    lr_scheduler_type=\"linear\",           # Use a linear learning rate scheduler\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usuario/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/usuario/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "# Paso 5: Inicializar el SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                          # The pre-trained and prepared model\n",
    "    train_dataset=tokenized_dataset['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_dataset['validation'],    # Evaluation dataset\n",
    "    peft_config=lora_config,              # LoRA configuration for efficient fine-tuning\n",
    "    max_seq_length=512,                   # Maximum sequence length for inputs\n",
    "    tokenizer=tokenizer,                  # Tokenizer for encoding the data\n",
    "    args=training_arguments,              # Training arguments defined earlier\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 52,912\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 13,107,200\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/usuario/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 1:41:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.478000</td>\n",
       "      <td>1.442956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.356000</td>\n",
       "      <td>1.402482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.276900</td>\n",
       "      <td>1.387287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.315400</td>\n",
       "      <td>1.383674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2756\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./v2_mistral7b_results/checkpoint-25\n",
      "loading configuration file config.json from cache at /home/usuario/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.3/snapshots/d8cadc02ac76bd617a919d50b092e59d2d110aff/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./v2_mistral7b_results/checkpoint-25/tokenizer_config.json\n",
      "Special tokens file saved in ./v2_mistral7b_results/checkpoint-25/special_tokens_map.json\n",
      "/home/usuario/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2756\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./v2_mistral7b_results/checkpoint-50\n",
      "loading configuration file config.json from cache at /home/usuario/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.3/snapshots/d8cadc02ac76bd617a919d50b092e59d2d110aff/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./v2_mistral7b_results/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./v2_mistral7b_results/checkpoint-50/special_tokens_map.json\n",
      "/home/usuario/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2756\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./v2_mistral7b_results/checkpoint-75\n",
      "loading configuration file config.json from cache at /home/usuario/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.3/snapshots/d8cadc02ac76bd617a919d50b092e59d2d110aff/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./v2_mistral7b_results/checkpoint-75/tokenizer_config.json\n",
      "Special tokens file saved in ./v2_mistral7b_results/checkpoint-75/special_tokens_map.json\n",
      "/home/usuario/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2756\n",
      "  Batch size = 2\n",
      "Saving model checkpoint to ./v2_mistral7b_results/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/usuario/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.3/snapshots/d8cadc02ac76bd617a919d50b092e59d2d110aff/config.json\n",
      "Model config MistralConfig {\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32768\n",
      "}\n",
      "\n",
      "tokenizer config file saved in ./v2_mistral7b_results/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./v2_mistral7b_results/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.3028882980346679, metrics={'train_runtime': 6116.2181, 'train_samples_per_second': 0.131, 'train_steps_per_second': 0.016, 'total_flos': 1.75151014477824e+16, 'train_loss': 1.3028882980346679, 'epoch': 0.015119443604475355})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 6: Iniciar el proceso de entrenamiento\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Instantiate the PerformanceBenchmark class with the model, tokenizer, and test dataset\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m PerformanceBenchmark(model, tokenizer, \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Run the benchmark to compute performance metrics\u001b[39;00m\n\u001b[1;32m    171\u001b[0m results \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mrun_benchmark()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy_score = load(\"accuracy\")\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    A class to benchmark the performance of a model on a given dataset.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    model : transformers.PreTrainedModel\n",
    "        The model to be benchmarked.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer associated with the model.\n",
    "    dataset : datasets.Dataset\n",
    "        The dataset on which the model's performance will be evaluated.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, dataset):\n",
    "        \"\"\"\n",
    "        Initializes the PerformanceBenchmark with the provided model, tokenizer, and dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : transformers.PreTrainedModel\n",
    "            The model to be benchmarked.\n",
    "        tokenizer : transformers.PreTrainedTokenizer\n",
    "            The tokenizer for encoding the inputs for the model.\n",
    "        dataset : datasets.Dataset\n",
    "            The dataset on which the model's performance will be evaluated.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def compute_parameters(self):\n",
    "        \"\"\"\n",
    "        Computes the total number of parameters and the number of trainable parameters.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing:\n",
    "            - `total_params`: The total number of parameters in the model.\n",
    "            - `trainable_params`: The number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())  # Total parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)  # Trainable parameters\n",
    "        \n",
    "        return {\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params\n",
    "        }\n",
    "\n",
    "    def compute_size(self):\n",
    "        \"\"\"\n",
    "        Computes the size of the model in terms of the number of parameters \n",
    "        and memory usage in megabytes (MB).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the number of parameters (`num_params`) and \n",
    "            the model size in MB (`model_size_mb`).\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        model_size_mb = sum(p.element_size() * p.nelement() for p in self.model.parameters()) / (1024**2)\n",
    "        \n",
    "        return {\"num_params\": num_params, \"model_size_mb\": model_size_mb}\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        \"\"\"\n",
    "        Measures the total time and average time taken by the model to process \n",
    "        the dataset.\n",
    "        \n",
    "        This method will use the tokenizer to encode the inputs before passing them \n",
    "        to the model.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the total processing time in seconds (`total_time_sec`) \n",
    "            and the average time per example (`avg_time_per_example_sec`).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for example in self.dataset:\n",
    "            inputs = example['conversations']\n",
    "            # Tokenize the input\n",
    "            tokenized_input = self.tokenizer(inputs, return_tensors=\"pt\").to(self.model.device)\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_example = total_time / len(self.dataset) if len(self.dataset) > 0 else float('inf')\n",
    "        \n",
    "        return {\"total_time_sec\": total_time, \"avg_time_per_example_sec\": avg_time_per_example}\n",
    "\n",
    "    def compute_latency(self):\n",
    "        \"\"\"\n",
    "        Computes the average latency of the model, defined as the time taken \n",
    "        to process a single example from the dataset.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the average latency in seconds (`avg_latency_sec`).\n",
    "        \"\"\"\n",
    "        latencies = []\n",
    "        \n",
    "        for example in self.dataset:\n",
    "            inputs = example['conversations']\n",
    "            # Tokenize the input\n",
    "            tokenized_input = self.tokenizer(inputs, return_tensors=\"pt\").to(self.model.device)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append(end_time - start_time)\n",
    "        \n",
    "        avg_latency = sum(latencies) / len(latencies) if len(latencies) > 0 else float('inf')\n",
    "        return {\"avg_latency_sec\": avg_latency}\n",
    "\n",
    "    def compute_throughput(self):\n",
    "        \"\"\"\n",
    "        Computes the throughput of the model, defined as the number of examples \n",
    "        processed per second.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the throughput in examples per second (`throughput_examples_per_sec`).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for example in self.dataset:\n",
    "            inputs = example['conversations']\n",
    "            # Tokenize the input\n",
    "            tokenized_input = self.tokenizer(inputs, return_tensors=\"pt\").to(self.model.device)\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        throughput = len(self.dataset) / total_time if total_time > 0 else 0\n",
    "        \n",
    "        return {\"throughput_examples_per_sec\": throughput}\n",
    "    \n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"\n",
    "        Runs all the benchmark metrics (size, time, latency, throughput, and FLOPs) \n",
    "        and returns the results.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing all the computed metrics for the model. \n",
    "            Includes size, parameters, time, latency, throughput, and FLOPs estimates.\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        metrics['Size'] = self.compute_size()\n",
    "        metrics['Parameters'] = self.compute_parameters()\n",
    "        metrics['Time'] = self.time_pipeline()\n",
    "        metrics['Latency'] = self.compute_latency()\n",
    "        metrics['Throughput'] = self.compute_throughput()\n",
    "        return metrics\n",
    "    \n",
    "# Instantiate the PerformanceBenchmark class with the model, tokenizer, and test dataset\n",
    "benchmark = PerformanceBenchmark(model, tokenizer, dataset['test'])\n",
    "\n",
    "# Run the benchmark to compute performance metrics\n",
    "results = benchmark.run_benchmark()\n",
    "\n",
    "# Display the benchmark results\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
