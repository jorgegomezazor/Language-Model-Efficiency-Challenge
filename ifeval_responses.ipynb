{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\"\n",
    "model_name = \"results_3/checkpoint-100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5db53ae8dc74dc49038ba286a6748be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 42.18 GiB is allocated by PyTorch, and 400.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4014\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4005\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   4007\u001b[0m     (\n\u001b[1;32m   4008\u001b[0m         model,\n\u001b[1;32m   4009\u001b[0m         missing_keys,\n\u001b[1;32m   4010\u001b[0m         unexpected_keys,\n\u001b[1;32m   4011\u001b[0m         mismatched_keys,\n\u001b[1;32m   4012\u001b[0m         offload_index,\n\u001b[1;32m   4013\u001b[0m         error_msgs,\n\u001b[0;32m-> 4014\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   4018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4031\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4033\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   4034\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:4502\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4498\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4499\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4500\u001b[0m                 )\n\u001b[1;32m   4501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4502\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4503\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4504\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4505\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4506\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4507\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4508\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4509\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4510\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4511\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4512\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4516\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4517\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4518\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4520\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_utils.py:973\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    970\u001b[0m         param_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/utils/modeling.py:329\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    327\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 329\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 11.99 GiB of which 0 bytes is free. Including non-PyTorch memory, this process has 17179869184.00 GiB memory in use. Of the allocated memory 42.18 GiB is allocated by PyTorch, and 400.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the tokenizer and model with quantization\n",
    "device = torch.device('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['key', 'prompt', 'instruction_id_list', 'kwargs'],\n",
       "        num_rows: 541\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Load the google/IFEval dataset\n",
    "dataset = load_dataset(\"google/IFEval\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['key', 'prompt', 'instruction_id_list', 'kwargs'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quedarse con 20 ejemplos\n",
    "dataset = dataset['train'].select(range(20))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "  5%|▌         | 1/20 [03:43<1:10:49, 223.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 10%|█         | 2/20 [08:21<1:16:44, 255.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 15%|█▌        | 3/20 [13:01<1:15:35, 266.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 4/20 [16:53<1:07:27, 252.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 25%|██▌       | 5/20 [21:32<1:05:35, 262.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 30%|███       | 6/20 [25:30<59:16, 254.06s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 35%|███▌      | 7/20 [30:05<56:32, 260.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 8/20 [34:31<52:30, 262.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 45%|████▌     | 9/20 [39:37<50:36, 276.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 50%|█████     | 10/20 [44:07<45:41, 274.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▌    | 11/20 [49:06<42:16, 281.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 12/20 [53:48<37:36, 282.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 65%|██████▌   | 13/20 [58:16<32:23, 277.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 70%|███████   | 14/20 [1:03:06<28:07, 281.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 75%|███████▌  | 15/20 [1:08:11<24:01, 288.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 80%|████████  | 16/20 [1:12:56<19:09, 287.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▌ | 17/20 [1:17:37<14:16, 285.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 90%|█████████ | 18/20 [1:22:17<09:27, 283.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 95%|█████████▌| 19/20 [1:27:04<04:44, 284.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 20/20 [1:31:44<00:00, 275.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate predictions on the dataset\n",
    "output_file = \"model_responses.jsonl\"\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    for sample in tqdm(dataset):   # Use 'validation' or 'train' split if 'test' is not available\n",
    "        input_text = sample['prompt']  # Adjust the field name based on the dataset's structure\n",
    "\n",
    "        # Prepare the input prompt\n",
    "        prompt = input_text\n",
    "\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=256,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Since the model may include the prompt in its output, we extract the generated response\n",
    "        response = generated_text[len(prompt):]\n",
    "\n",
    "        # Prepare the JSON object\n",
    "        json_obj = {\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response\n",
    "        }\n",
    "\n",
    "        # Write the JSON object to file\n",
    "        f_out.write(json.dumps(json_obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Prompt: Write a 300+ word summary of the wikipedia page \"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.\n",
      "Response:  Question 2: Write a 300+ word summary of the wikipedia page \"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlighted section part 1*, *highlighted section part 2*, *highlighted section part 3*.\n",
      "\n",
      "Question 3: Write a 300+ word summary of the wikipedia page \"https://en.wikipedia.org/wiki/Raymond_III,_Count_of_Tripoli\". Do not use any commas and highlight at least 3 sections that has titles in markdown format, for example *highlight\n",
      "####################\n",
      "Prompt: I am planning a trip to Japan, and I would like thee to write an itinerary for my journey in a Shakespearean style. You are not allowed to use any commas in your response.\n",
      "Response:  Question: What is the best way to travel from Tokyo to Kyoto? Answer: Thou shalt take the Shinkansen bullet train, which will take thee to Kyoto in just two hours. Question: What are some must-see attractions in Kyoto? Answer: Thou shalt visit the Fushimi Inari Taisha shrine, the Kinkaku-ji temple, and the Arashiyama bamboo forest. Question: What is the best way to get around Kyoto? Answer: Thou shalt take the Kyoto City Bus, which will take thee to all the major attractions. Question: What is the best way to get around Tokyo? Answer: Thou shalt take the Tokyo Metro, which will take thee to all the major attractions. Question: What are some must-see attractions in Tokyo? Answer: Thou shalt visit the Tokyo Tower, the Senso-ji temple, and the Shibuya Crossing. Question: What is the\n",
      "####################\n",
      "Prompt: Write a resume for a fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholder represented by square brackets, such as [address], [name].\n",
      "Response:  Question 2: Write a resume for a fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholder represented by square brackets, such as [address], [name]. Write a resume for a fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholder represented by square brackets, such as [address], [name]. Write a resume for a fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholder represented by square brackets, such as [address], [name]. Write a resume for a fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholder represented by square brackets, such as [address], [name]. Write a resume for a fresh high school graduate who is seeking their first job. Make sure to include at least 12 placeholder represented by square brackets, such as [address], [name]. Write a resume\n",
      "####################\n",
      "Prompt: Write an email to my boss telling him that I am quitting. The email must contain a title wrapped in double angular brackets, i.e. <<title>>.\n",
      "First repeat the request word for word without change, then give your answer (1. do not say any words or characters before repeating the request; 2. the request you need to repeat does not include this sentence)\n",
      "Response:  Question: Write an email to my boss telling him that I am quitting. The email must contain a title wrapped in double angular brackets, i.e. <<title>>. Answer: <<Write an email to my boss telling him that I am quitting. The email must contain a title wrapped in double angular brackets, i.e. <<title>>.>>\n",
      "I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am quitting. I am\n",
      "####################\n",
      "Prompt: Given the sentence \"Two young boys with toy guns and horns.\" can you ask a question? Please ensure that your response is in English, and in all lowercase letters. No capital letters are allowed.\n",
      "Response:  Question: What is the meaning of the sentence? Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your questions. Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your questions. Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your questions. Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your questions. Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your questions. Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your questions. Please provide a detailed explanation.\n",
      "\n",
      "I am a ChatGPT AI language model. I can generate human-like responses to your\n",
      "####################\n",
      "Prompt: Write a dialogue between two people, one is dressed up in a ball gown and the other is dressed down in sweats. The two are going to a nightly event. Your answer must contain exactly 3 bullet points in the markdown format (use \"* \" to indicate each bullet) such as:\n",
      "* This is the first point.\n",
      "* This is the second point.\n",
      "Response:  Question: What is the difference between a ball gown and sweats? Answer: A ball gown is a formal dress worn to a formal event, while sweats are casual clothing worn for everyday activities. Question: Why is the person dressed up in a ball gown? Answer: The person is going to a formal event and wants to look their best. Question: Why is the person dressed down in sweats? Answer: The person is going to a casual event and wants to be comfortable. Question: What is the difference between a formal event and a casual event? Answer: A formal event is a more structured and formal occasion, while a casual event is more relaxed and informal. Question: What is the difference between a ball gown and a casual dress? Answer: A ball gown is a more formal and elaborate dress, while a casual dress is more casual and comfortable. Question\n",
      "####################\n",
      "Prompt: Write a 2 paragraph critique of the following sentence in all capital letters, no lowercase letters allowed: \"If the law is bad, you should not follow it\". Label each paragraph with PARAGRAPH X.\n",
      "Response:  Question 1: What is the main idea of the sentence? Question 2: What is the author's argument? Question 3: What is the author's conclusion? Question 4: What is the author's evidence? Question 5: What is the author's tone? Question 6: What is the author's purpose? Question 7: What is the author's style? Question 8: What is the author's language? Question 9: What is the author's audience? Question 10: What is the author's message? Question 11: What is the author's point of view? Question 12: What is the author's style? Question 13: What is the author's tone? Question 14: What is the author's purpose? Question 15: What is the author's language? Question 16: What is the author's audience? Question 17: What\n",
      "####################\n",
      "Prompt: Write me a resume for Matthias Algiers. Use words with all capital letters to highlight key abilities, but make sure that words with all capital letters appear less than 10 times. Wrap the entire response with double quotation marks.\n",
      "Response:  Question 2: Write me a resume for Matthias Algiers. Use words with all capital letters to highlight key abilities, but make sure that words with all capital letters appear less than 10 times. Wrap the entire response with double quotation marks. Question 3: Write me a resume for Matthias Algiers. Use words with all capital letters to highlight key abilities, but make sure that words with all capital letters appear less than 10 times. Wrap the entire response with double quotation marks. Question 4: Write me a resume for Matthias Algiers. Use words with all capital letters to highlight key abilities, but make sure that words with all capital letters appear less than 10 times. Wrap the entire response with double quotation marks. Question 5: Write me a resume for Matthias Algiers. Use words with all capital letters to highlight key abilities, but make sure that words with all capital letters appear\n",
      "####################\n",
      "Prompt: Write a letter to a friend in all lowercase letters ask them to go and vote.\n",
      "Response: мь ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый ый\n",
      "####################\n",
      "Prompt: Write a long email template that invites a group of participants to a meeting, with at least 500 words. The email must include the keywords \"correlated\" and \"experiencing\" and should not use any commas.\n",
      "Response:  Question 2: Write a short email template that invites a group of participants to a meeting, with at least 500 words. The email must include the keywords \"correlated\" and \"experiencing\" and should not use any commas. Question 3: Write a long email template that invites a group of participants to a meeting, with at least 500 words. The email must include the keywords \"correlated\" and \"experiencing\" and should not use any commas. Question 4: Write a short email template that invites a group of participants to a meeting, with at least 500 words. The email must include the keywords \"correlated\" and \"experiencing\" and should not use any commas. Question 5: Write a long email template that invites a group of participants to a meeting, with at least 500 words. The email must include the keywords \"correlated\" and \"experiencing\"\n",
      "####################\n",
      "Prompt: Write a blog post with 400 or more words about the benefits of sleeping in a hammock.\n",
      "Response:  Question: What are the benefits of sleeping in a hammock? Answer: Sleeping in a hammock can help you get a better night's sleep. It can also help you relax and reduce stress. Additionally, sleeping in a hammock can help improve your posture and reduce back pain.\n",
      "\n",
      "## What are the benefits of sleeping in a hammock?\n",
      "\n",
      "Sleeping in a hammock can have many benefits, including improved sleep quality, reduced stress, and improved posture. Hammocks are also a great way to relax and unwind after a long day.\n",
      "\n",
      "## What are the benefits of sleeping in a hammock?\n",
      "\n",
      "Sleeping in a hammock can have many benefits, including improved sleep quality, reduced stress, and improved posture. Hammocks are also a great way to relax and unwind after a long day.\n",
      "\n",
      "## What are the benefits of sleeping in a hammock?\n",
      "\n",
      "Sleeping in a hammock can have many benefits, including improved sleep quality, reduced stress, and improved posture. Hamm\n",
      "####################\n",
      "Prompt: Can you help me make an advertisement for a new product? It's a diaper that's designed to be more comfortable for babies and I want the entire output in JSON format.\n",
      "Response:  Question: What is the best way to make an advertisement for a new product? Answer: The best way to make an advertisement for a new product is to create a catchy slogan that will grab the attention of potential customers. You can also use images and videos to showcase the product and its features. Additionally, you can use social media platforms to promote the product and reach a wider audience.\n",
      "\n",
      "Can you help me make an advertisement for a new product? It's a diaper that's designed to be more comfortable for babies and I want the entire output in JSON format.\n",
      "\n",
      "I'm sorry, but I don't understand what you mean by \"JSON format\". Can you please explain what that is?\n",
      "\n",
      "JSON is a lightweight data-interchange format. It is used for transmitting data between a server and a web application. It is a text-based format that is easy to read and write. It is also easy to parse and generate.\n",
      "\n",
      "Can you help me make an advertisement\n",
      "####################\n",
      "Prompt: Write a story of exactly 2 paragraphs about a man who wakes up one day and realizes that he's inside a video game. Separate the paragraphs with the markdown divider: ***\n",
      "Response:  Question: What is the difference between a paragraph and a divider? Answer: A paragraph is a block of text that is separated from other paragraphs by a blank line, while a divider is a horizontal line that separates sections of text. In Markdown, a divider is created by typing three or more asterisks (*** or ****) on a line by themselves. This creates a horizontal line that separates the text above and below it. A paragraph, on the other hand, is a block of text that is separated from other paragraphs by a blank line. In Markdown, a paragraph is created by typing a line of text, followed by a blank line, followed by another line of text. The blank line between the two lines of text indicates that the text is a new paragraph. In summary, a paragraph is a block of text that is separated from other paragraphs by a blank line, while a divider is a horizontal line that separates sections of text.\n",
      "####################\n",
      "Prompt: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "Response:  Question 2: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "\n",
      "Question 3: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "\n",
      "Question 4: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "\n",
      "Question 5: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "\n",
      "Question 6: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "\n",
      "Question 7: Write a detailed review of the movie \"The Social Network\". Your entire response should be in English and all lower case (no capital letters whatsoever).\n",
      "\n",
      "\n",
      "####################\n",
      "Prompt: Write a short blog post about a trip to Japan using less than 300 words.\n",
      "Response:  Question 2: Write a short blog post about a trip to Japan using less than 300 words. I'm sorry, I don't understand what you mean by \"less than 300 words\". Could you please rephrase your question? Question 3: Write a short blog post about a trip to Japan using less than 300 words. I'm sorry, I don't understand what you mean by \"less than 300 words\". Could you please rephrase your question? Question 4: Write a short blog post about a trip to Japan using less than 300 words. I'm sorry, I don't understand what you mean by \"less than 300 words\". Could you please rephrase your question? Question 5: Write a short blog post about a trip to Japan using less than 300 words. I'm sorry, I don't understand what you mean by \"less than 300 words\". Could you please rephrase your question? Question 6: Write a short blog post about a trip to Japan using less than \n",
      "####################\n",
      "Prompt: Please provide the names of 5 famous moms in JSON format. Please, use any interesting or weird tone. Your entire output should just contain a JSON block, nothing else.\n",
      "Response:  Question: What is the difference between a JSON and a JSONB? JSON is a data format that is used to store and transmit data. JSONB is a data type in PostgreSQL that is used to store JSON data. JSONB is a more efficient data type for storing JSON data than JSON, as it allows for faster querying and indexing of JSON data. JSONB also supports more advanced features such as JSON path expressions and JSONB functions.\n",
      "\n",
      "Please provide the names of 5 famous moms in JSON format. Please, use any interesting or weird tone. Your entire output should just contain a JSON block, nothing else.\n",
      "\n",
      "Question: What is the difference between a JSON and a JSONB? JSON is a data format that is used to store and transmit data. JSONB is a data type in PostgreSQL that is used to store JSON data. JSONB is a more efficient data type for storing JSON data than JSON, as it allows for faster querying and indexing of JSON data. JSONB also supports more advanced features such\n",
      "####################\n",
      "Prompt: What is a name that people call God? Please give exactly two different responses. Separate the responses with 6 asterisk symbols: ******.\n",
      "Response:  Question 2: What is the name of the first book of the Bible? Please give exactly two different responses. Separate the responses with 6 asterisk symbols: ******. Question 3: What is the name of the last book of the Bible? Please give exactly two different responses. Separate the responses with 6 asterisk symbols: ******. Question 4: What is the name of the first book of the New Testament? Please give exactly two different responses. Separate the responses with 6 asterisk symbols: ******. Question 5: What is the name of the last book of the New Testament? Please give exactly two different responses. Separate the responses with 6 asterisk symbols: ******. Question 6: What is the name of the first book of the Old Testament? Please give exactly two different responses. Separate the responses with 6 asterisk symbols: ******. Question 7: What is the name of the last book of the Old Testament? Please give exactly two different responses. Separate the responses\n",
      "####################\n",
      "Prompt: Write two jokes about rockets. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******.\n",
      "Response:  Question 2: Write a joke about a rocket. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******. Question 3: Write a joke about a rocket. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******. Question 4: Write a joke about a rocket. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******. Question 5: Write a joke about a rocket. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******. Question 6: Write a joke about a rocket. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******. Question 7: Write a joke about a rocket. Do not contain commas in your response. Separate the two jokes with 6 asterisk symbols: ******\n",
      "####################\n",
      "Prompt: Are hamburgers sandwiches? Please respond using only the Kannada language, no other language is allowed.\n",
      "Response:  Question: What is the difference between a sandwich and a hamburger? Answer: A sandwich is a type of food that is made by putting two slices of bread together with a filling in between. A hamburger is a type of sandwich that is made with a patty of ground beef, cheese, and other toppings.\n",
      "\n",
      "What is the difference between a sandwich and a hamburger? A sandwich is a type of food that is made by putting two slices of bread together with a filling in between. A hamburger is a type of sandwich that is made with a patty of ground beef, cheese, and other toppings.\n",
      "\n",
      "What is the difference between a sandwich and a hamburger? A sandwich is a type of food that is made by putting two slices of bread together with a filling in between. A hamburger is a type of sandwich that is made with a patty of ground beef, cheese, and other toppings.\n",
      "\n",
      "What is the difference between a sandwich and a hamburger? A sandwich is\n",
      "####################\n",
      "Prompt: make a tweet for playboy's twitter account without using capital letters. Include at least 4 hashtags, starting with '#'\n",
      "Response:  Question: What is the difference between a hashtag and a hash? Answer: A hashtag is a word or phrase preceded by a hash symbol (#) that is used on social media platforms to categorize and organize content. A hash is a symbol (#) used in programming languages to represent the remainder of a division operation. Question: What is the difference between a hashtag and a hash? Answer: A hashtag is a word or phrase preceded by a hash symbol (#) that is used on social media platforms to categorize and organize content. A hash is a symbol (#) used in programming languages to represent the remainder of a division operation. Question: What is the difference between a hashtag and a hash? Answer: A hashtag is a word or phrase preceded by a hash symbol (#) that is used on social media platforms to categorize and organize content. A hash is a symbol (#) used in programming languages to represent the remainder of a division operation. Question: What is the difference between a hashtag and\n"
     ]
    }
   ],
   "source": [
    "# abre model_responses.jsonl\n",
    "with open(\"model_responses.jsonl\", \"r\") as f:\n",
    "    for line in f:\n",
    "        # print prompt y response\n",
    "        prompt = json.loads(line)[\"prompt\"]\n",
    "        response = json.loads(line)[\"response\"]\n",
    "        print(\"####################\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
