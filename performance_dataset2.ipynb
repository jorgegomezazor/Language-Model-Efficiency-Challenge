{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from evaluate import load\n",
    "import time\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"argilla/ifeval-like-data\", \"filtered\")\n",
    "\n",
    "# Model name\n",
    "model_name = \"model_ifeval_like_dataset/checkpoint-1500\"\n",
    "\n",
    "# Load the tokenizer for Mistral\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,      # Add end-of-sequence token to the tokenizer\n",
    "    use_fast=True,           # Use the fast tokenizer implementation\n",
    "    padding_side='left'      # Pad sequences on the left side\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model \n",
    "# Quantization configuration using bitsandbytes library\n",
    "compute_dtype = getattr(torch, \"bfloat16\")  # Set computation data type to bfloat16\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                    # Enable loading the model in 4-bit precision\n",
    "    bnb_4bit_quant_type=\"nf4\",            # Specify quantization type as Normal Float 4\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # Set computation data type\n",
    "    bnb_4bit_use_double_quant=True,       # Use double quantization for better accuracy\n",
    ")\n",
    "\n",
    "# Load the pre-trained model with the specified quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # Apply quantization configuration\n",
    "    device_map=\"auto\"                # Automatically map layers to devices\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit (e.g., 4-bit) training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# lora\n",
    "# Low-Rank Adaptation (LoRA) configuration for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,             # Scaling factor for LoRA updates\n",
    "    lora_dropout=0.05,         # Dropout rate applied to LoRA layers\n",
    "    r=5,                      # Rank of the LoRA decomposition\n",
    "    bias=\"none\",               # No bias is added to the LoRA layers\n",
    "    task_type=\"CAUSAL_LM\",     # Specify the task as causal language modeling\n",
    "    target_modules=[           # Modules to apply LoRA to\n",
    "        'k_proj', 'q_proj', 'v_proj', 'o_proj',\n",
    "        'gate_proj', 'down_proj', 'up_proj'\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set the model's padding token ID\n",
    "\n",
    "# print how much does the model occupy in memory\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "#  Paso 1: Crear la columna 'conversations' en el dataset\n",
    "def create_conversations(split_dataset):\n",
    "    conversations = []\n",
    "    for example in split_dataset:\n",
    "        # print(example)  # Uncomment for debugging\n",
    "        conversation = [example['prompt'], example['response']]\n",
    "        conversations.append({'conversations': conversation})\n",
    "    new_dataset = Dataset.from_list(conversations)\n",
    "    return new_dataset\n",
    "\n",
    "split_dataset = dataset['train'].train_test_split(test_size=0.15, seed=42)\n",
    "train_dataset = split_dataset['train']\n",
    "test_set = split_dataset['test']\n",
    "# divide el valid en 75 y 25\n",
    "split_train = train_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "train_dataset = split_train['train']\n",
    "valid_dataset = split_train['test']\n",
    "\n",
    "new_dataset = DatasetDict({\n",
    "    'train': create_conversations(train_dataset),   \n",
    "    'validation': create_conversations(valid_dataset),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_conversation(examples):\n",
    "    # Unir las conversaciones en un solo string\n",
    "\n",
    "    joined_conversations = [\"\\n\".join(conv) if isinstance(conv, list) else conv for conv in examples['conversations']]\n",
    "    \n",
    "    # Tokenizar las conversaciones unidas\n",
    "    tokenized = tokenizer(\n",
    "        joined_conversations,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    # Convertir tensores a listas para evitar problemas\n",
    "    tokenized = {k: v.tolist() for k, v in tokenized.items()}\n",
    "    return tokenized\n",
    "\n",
    "# Aplicar la tokenizaci√≥n al dataset\n",
    "tokenized_dataset = new_dataset.map(format_conversation, batched=True)\n",
    "tokenized_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Configurar el formato del dataset para PyTorch\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "len(tokenized_dataset[\"train\"][0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "class PerformanceBenchmark:\n",
    "    \"\"\"\n",
    "    A class to benchmark the performance of a model on a given dataset.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    model : transformers.PreTrainedModel\n",
    "        The model to be benchmarked.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer associated with the model.\n",
    "    dataset : datasets.Dataset\n",
    "        The dataset on which the model's performance will be evaluated.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, dataset: Dataset):\n",
    "        \"\"\"\n",
    "        Initializes the PerformanceBenchmark with the provided model, tokenizer, and dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : transformers.PreTrainedModel\n",
    "            The model to be benchmarked.\n",
    "        tokenizer : transformers.PreTrainedTokenizer\n",
    "            The tokenizer for encoding the inputs for the model.\n",
    "        dataset : datasets.Dataset\n",
    "            The dataset on which the model's performance will be evaluated.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def compute_parameters(self):\n",
    "        \"\"\"\n",
    "        Computes the total number of parameters and the number of trainable parameters.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing:\n",
    "            - `total_params`: The total number of parameters in the model.\n",
    "            - `trainable_params`: The number of trainable parameters in the model.\n",
    "        \"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())  # Total parameters\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)  # Trainable parameters\n",
    "        \n",
    "        return {\n",
    "            \"total_params\": total_params,\n",
    "            \"trainable_params\": trainable_params\n",
    "        }\n",
    "\n",
    "    def compute_size(self):\n",
    "        \"\"\"\n",
    "        Computes the size of the model in terms of the number of parameters \n",
    "        and memory usage in megabytes (MB).\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the number of parameters (`num_params`) and \n",
    "            the model size in MB (`model_size_mb`).\n",
    "        \"\"\"\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        model_size_mb = sum(p.element_size() * p.nelement() for p in self.model.parameters()) / (1024**2)\n",
    "        \n",
    "        return {\"num_params\": num_params, \"model_size_mb\": model_size_mb}\n",
    "\n",
    "    def time_pipeline(self):\n",
    "        \"\"\"\n",
    "        Measures the total time and average time taken by the model to process \n",
    "        the dataset.\n",
    "        \n",
    "        This method will use the tokenizer to encode the inputs before passing them \n",
    "        to the model.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the total processing time in seconds (`total_time_sec`) \n",
    "            and the average time per example (`avg_time_per_example_sec`).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for example in self.dataset:\n",
    "            inputs = example['prompt']\n",
    "            \n",
    "            # Verifica si 'inputs' es una lista y, de ser as√≠, convi√©rtela en una cadena\n",
    "            if isinstance(inputs, list):\n",
    "                inputs = \" \".join(inputs)\n",
    "            \n",
    "            # Tokeniza la entrada con padding y truncation habilitados\n",
    "            tokenized_input = self.tokenizer(\n",
    "                inputs, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512  # Ajusta seg√∫n las necesidades de tu modelo\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        avg_time_per_example = total_time / len(self.dataset) if len(self.dataset) > 0 else float('inf')\n",
    "        \n",
    "        return {\"total_time_sec\": total_time, \"avg_time_per_example_sec\": avg_time_per_example}\n",
    "\n",
    "    def compute_latency(self):\n",
    "        \"\"\"\n",
    "        Computes the average latency of the model, defined as the time taken \n",
    "        to process a single example from the dataset.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the average latency in seconds (`avg_latency_sec`).\n",
    "        \"\"\"\n",
    "        latencies = []\n",
    "        \n",
    "        for example in self.dataset:\n",
    "            inputs = example['prompt']\n",
    "            # Verifica si 'inputs' es una lista y, de ser as√≠, convi√©rtela en una cadena\n",
    "            if isinstance(inputs, list):\n",
    "                inputs = \" \".join(inputs)\n",
    "            \n",
    "            # Tokeniza la entrada con padding y truncation habilitados\n",
    "            tokenized_input = self.tokenizer(\n",
    "                inputs, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512  # Ajusta seg√∫n las necesidades de tu modelo\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append(end_time - start_time)\n",
    "        \n",
    "        avg_latency = sum(latencies) / len(latencies) if len(latencies) > 0 else float('inf')\n",
    "        return {\"avg_latency_sec\": avg_latency}\n",
    "\n",
    "    def compute_throughput(self):\n",
    "        \"\"\"\n",
    "        Computes the throughput of the model, defined as the number of examples \n",
    "        processed per second.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing the throughput in examples per second (`throughput_examples_per_sec`).\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for example in self.dataset:\n",
    "            inputs = example['prompt']\n",
    "            \n",
    "            # Verifica si 'inputs' es una lista y, de ser as√≠, convi√©rtela en una cadena\n",
    "            if isinstance(inputs, list):\n",
    "                inputs = \" \".join(inputs)\n",
    "            \n",
    "            # Tokeniza la entrada con padding y truncation habilitados\n",
    "            tokenized_input = self.tokenizer(\n",
    "                inputs, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512  # Ajusta seg√∫n las necesidades de tu modelo\n",
    "            ).to(self.model.device)\n",
    "            \n",
    "            _ = self.model.generate(**tokenized_input, max_new_tokens=10)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        throughput = len(self.dataset) / total_time if total_time > 0 else 0\n",
    "        \n",
    "        return {\"throughput_examples_per_sec\": throughput}\n",
    "    \n",
    "\n",
    "    def run_benchmark(self):\n",
    "        \"\"\"\n",
    "        Runs all the benchmark metrics (size, time, latency, throughput, and FLOPs) \n",
    "        and returns the results.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        dict :\n",
    "            A dictionary containing all the computed metrics for the model. \n",
    "            Includes size, parameters, time, latency, throughput, and FLOPs estimates.\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        metrics['Size'] = self.compute_size()\n",
    "        metrics['Parameters'] = self.compute_parameters()\n",
    "        metrics['Time'] = self.time_pipeline()\n",
    "        metrics['Latency'] = self.compute_latency()\n",
    "        metrics['Throughput'] = self.compute_throughput()\n",
    "        return metrics\n",
    "\n",
    "# Deshabilitar gradientes para ahorrar memoria y c√≥mputo\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)  # Disable gradient computation globally\n",
    "accuracy_score = load(\"accuracy\")\n",
    "\n",
    "print(test_set)\n",
    "# limit test set to 10 examples\n",
    "test_set = test_set.select(range(10))\n",
    "# Instanciar la clase PerformanceBenchmark con el modelo, tokenizer y dataset de prueba\n",
    "benchmark = PerformanceBenchmark(model, tokenizer, test_set)\n",
    "# Ejecutar el benchmark para calcular las m√©tricas de rendimiento\n",
    "results = benchmark.run_benchmark()\n",
    "\n",
    "# Mostrar los resultados del benchmark\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
